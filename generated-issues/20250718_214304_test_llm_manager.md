# FEATURE: Test LLM Manager

## EXECUTIVE SUMMARY
This feature is about ensuring the `LLMManager` is robust, well-tested, and properly integrated into the system. This involves creating comprehensive unit and integration tests to validate its functionality, error handling, and interaction with other components.

## CODEBASE ANALYSIS
- **`core/llm_manager.py`**: This is the core component to be tested. It handles interactions with different LLM providers, manages configuration, and executes LLM calls. The current implementation supports only the Gemini provider and relies on an environment variable for the API key.
- **`tests/core/test_llm_manager.py`**: A unit test file already exists for `LLMManager`. It mocks `ConfigManager` and the `google.generativeai` library to test the manager's logic in isolation. It covers initialization, configuration, and basic success/failure scenarios.
- **`tests/integration_test_llm_manager.py`**: This file contains basic integration tests. It verifies that `LLMManager` is correctly instantiated by `AgentOrchestrator` and that `BaseAgent` can accept an `LLMManager` instance. It does not perform actual LLM calls.
- **`requirements.txt`**: This file is missing the `google-generativeai` dependency, which is required for the `LLMManager` to function.

## DOMAIN RESEARCH
- **User Workflows**: Developers using this system will rely on the `LLMManager` for all AI-powered features. Its reliability is critical. If the manager fails, the entire system's core functionality is compromised.
- **Industry Patterns**: It is standard practice to have a dedicated manager or service for interacting with external APIs like LLMs. This isolates dependencies and provides a single point of control for configuration, error handling, and mocking in tests. The current implementation follows this pattern.
- **Competitive Analysis**: Similar systems often have more advanced features like automatic retries, provider fallbacks, and cost tracking. These are out of scope for this feature but are potential future enhancements.

## TECHNICAL APPROACH
The recommended approach is to enhance the existing tests to be more comprehensive. This includes:
1.  **Adding `google-generativeai` to `requirements.txt`**: This is a necessary dependency.
2.  **Improving Unit Tests**:
    *   Add more specific tests for the `_ensure_client_initialized` method to verify lazy initialization.
    *   Test the `execute_llm_call` method with different temperature settings.
    *   Test the error handling when the LLM call itself fails.
3.  **Improving Integration Tests**:
    *   Create a new integration test that makes a real (but simple) call to the Gemini API. This will require setting the `GEMINI_API_KEY` in the test environment. This test should be clearly marked as requiring an API key and network access.

## IMPLEMENTATION SPECIFICATION
### Database Changes
None.

### API Design
None.

### Frontend Components
None.

### Backend Services
- **`LLMManager`**: No changes to the class itself, but its tests will be improved.

## RISK ASSESSMENT
### Technical Risks
- **Dependency on External Service**: The integration test will depend on the availability of the Gemini API.
  - **Mitigation**: This test can be skipped in environments where network access or API keys are not available.
- **API Key Security**: The integration test will require an API key.
  - **Mitigation**: The API key will be loaded from an environment variable, not hardcoded in the source. The test instructions will make it clear that a test-specific, low-privilege key should be used.

### Business Risks
- **Cost**: Running integration tests that call the LLM API will incur costs.
  - **Mitigation**: The test will be designed to make a minimal, low-cost call. It will not be part of the standard CI/CD pipeline by default, but can be run manually.

## PROJECT DETAILS
**Estimated Effort**: 1 day
**Dependencies**: None
**Priority**: High
**Category**: technical-debt

## IMPLEMENTATION DETAILS
- **Files to create/modify**:
    - `tests/core/test_llm_manager.py`: Add more detailed unit tests.
    - `tests/integration_test_llm_manager.py`: Add a real API call test.
    - `requirements.txt`: Add `google-generativeai`.
- **Key classes/functions to implement**:
    - `TestLLMManager.test_lazy_initialization`
    - `TestLLMManager.test_llm_call_failure`
    - `integration_test_llm_manager.test_real_api_call`
- **Exact CLI command structure**:
  ```bash
  python3 -m unittest tests/core/test_llm_manager.py
  GEMINI_API_KEY="your_api_key" python3 tests/integration_test_llm_manager.py
  ```
- **Clear acceptance criteria for "done"**:
    - All existing and new unit tests for `LLMManager` pass.
    - The new integration test successfully makes an API call to Gemini and gets a valid response.
    - `requirements.txt` is updated.

## SCOPE BOUNDARIES
- **IN SCOPE**:
    - Writing unit and integration tests for the existing `LLMManager`.
    - Adding the necessary dependency to `requirements.txt`.
- **OUT OF SCOPE**:
    - Adding new features to `LLMManager` like provider fallbacks or cost tracking.
    - Adding support for other LLM providers.
    - Setting up a full CI/CD pipeline for these tests.

## ACCEPTANCE CRITERIA
- [ ] `google-generativeai` is added to `requirements.txt`.
- [ ] Unit tests for `LLMManager` are comprehensive and cover all public methods and error conditions.
- [ ] An integration test exists that can successfully execute a real LLM call using the Gemini provider.

## GITHUB ISSUE TEMPLATE
**Title**: Test LLM Manager
**Labels**: testing, technical-debt
**Assignee**:
**Project**: Loom Phase 1
